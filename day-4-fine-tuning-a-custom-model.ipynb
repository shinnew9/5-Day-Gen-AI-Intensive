{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# @title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:14:58.942737Z","iopub.execute_input":"2025-04-03T17:14:58.943056Z","iopub.status.idle":"2025-04-03T17:14:58.947353Z","shell.execute_reply.started":"2025-04-03T17:14:58.943031Z","shell.execute_reply":"2025-04-03T17:14:58.946349Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Day 4 - Fine tuning a custom model\n\nWelcome back to the Kaggle 5-day Generative AI course!\n\nIn this notebook you will use the Gemini API to fine-tune a custom, task-specific model. Fine-tuning can be used for a variety of tasks from classic NLP problems like entity extraction or summarisation, to creative tasks like stylised generation. You will fine-tune a model to classify the category t piece of text (a newsgroup post) into the category it belongs to (the newsgroup name).\n\nThis codelab walks you tuning a model with the API. [AI Studio](https://aistudio.google.com/app/tune) also supports creating a new tuned models directly in the webUI, allowing you to quickly create and monitor models using data from Google Sheets, Drive or your own files.\n\n**Note**: We recommend doing this codelab first doday. There are may be a period of waiting while the model tunes, so if you start this one, you can try the other codelab while you wait.","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:14:58.949076Z","iopub.execute_input":"2025-04-03T17:14:58.949427Z","iopub.status.idle":"2025-04-03T17:15:05.610983Z","shell.execute_reply.started":"2025-04-03T17:14:58.949396Z","shell.execute_reply":"2025-04-03T17:15:05.609915Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:05.612624Z","iopub.execute_input":"2025-04-03T17:15:05.612933Z","iopub.status.idle":"2025-04-03T17:15:06.815123Z","shell.execute_reply.started":"2025-04-03T17:15:05.612902Z","shell.execute_reply":"2025-04-03T17:15:06.814185Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'1.7.0'"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### Set up your API key\n\nTo run the following cell, your API key must be stroed it in a [Kaggle secret](https://www.kaggle.com/discussions/product-feedback/114053) named `GOOGLE_API_KEY`.\n\nIf you don't already have an API key, you can grab one from [AI Studio](https://aistudio.google.com/app/apikey). You can find [detailed instructions in the docs](https://ai.google.dev/gemini-api/docs/api-key).\n\nTo make the key available through Kaggle secrets, choose `Secrets` from the `Add-ons` menu and follow the instructions to add your key or enable it for this notebook.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\nclient = genai.Client(api_key=GOOGLE_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:06.816064Z","iopub.execute_input":"2025-04-03T17:15:06.816516Z","iopub.status.idle":"2025-04-03T17:15:07.217210Z","shell.execute_reply.started":"2025-04-03T17:15:06.816489Z","shell.execute_reply":"2025-04-03T17:15:07.216548Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"If you received an error response along the lines of `No user secrets exist for kernel id ...`, then you need to add your API key via `Add-ons`, `Secrets` **and** enable it.\n\n![Screenshot of the checkbox to enable GOOGLE_API_KEY secret](https://storage.googleapis.com/kaggle-media/Images/5gdai_sc_3.png)","metadata":{}},{"cell_type":"markdown","source":"### Explore available models\n\nYou will be using the [`TunedModel.create`](https://ai.google.dev/api/tuning#method:-tunedmodels.create) API moethod to start the fine-tuning job and create your custom model. Finda model that supports it through the [`models.list`](https://ai.google.dev/api/models#method:-models.list) endpoint. You can also find more information about tuning models in  [the model tuning docs](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python).","metadata":{}},{"cell_type":"code","source":"for model in client.models.list():\n    if \"creatTunedModel\" in model.supported_actions:\n        print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:07.217950Z","iopub.execute_input":"2025-04-03T17:15:07.218146Z","iopub.status.idle":"2025-04-03T17:15:07.456740Z","shell.execute_reply.started":"2025-04-03T17:15:07.218128Z","shell.execute_reply":"2025-04-03T17:15:07.455885Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Download the dataset\n\nIn this activity, you will use the same newsgroups dataset that [you used to train a classifier in Keras](https://www.kaggle.com/code/markishere/day-2-classifying-embeddings-with-keras/). In this example you will use a fine-tuned Gemini model to acheive the same goal.\n\nThe [20 Newsgroups Text Dataset](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) contains 18,000 newsgroups posts on 20 topics divided into training and test stes.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_20newsgroups\n\nnewsgroups_train = fetch_20newsgroups(subset=\"train\")\nnewsgroups_test = fetch_20newsgroups(subset=\"test\")\n\n# View list of class names for dataset\nnewsgroups_train.target_names","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:07.458660Z","iopub.execute_input":"2025-04-03T17:15:07.458883Z","iopub.status.idle":"2025-04-03T17:15:17.966017Z","shell.execute_reply.started":"2025-04-03T17:15:07.458863Z","shell.execute_reply":"2025-04-03T17:15:17.965130Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['alt.atheism',\n 'comp.graphics',\n 'comp.os.ms-windows.misc',\n 'comp.sys.ibm.pc.hardware',\n 'comp.sys.mac.hardware',\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'sci.crypt',\n 'sci.electronics',\n 'sci.med',\n 'sci.space',\n 'soc.religion.christian',\n 'talk.politics.guns',\n 'talk.politics.mideast',\n 'talk.politics.misc',\n 'talk.religion.misc']"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"Here's what a sinlge row looks like.","metadata":{}},{"cell_type":"code","source":"print(newsgroups_train.data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:17.967731Z","iopub.execute_input":"2025-04-03T17:15:17.968031Z","iopub.status.idle":"2025-04-03T17:15:17.973225Z","shell.execute_reply.started":"2025-04-03T17:15:17.968005Z","shell.execute_reply":"2025-04-03T17:15:17.971947Z"}},"outputs":[{"name":"stdout","text":"From: lerxst@wam.umd.edu (where's my thing)\nSubject: WHAT car is this!?\nNntp-Posting-Host: rac3.wam.umd.edu\nOrganization: University of Maryland, College Park\nLines: 15\n\n I was wondering if anyone out there could enlighten me on this car I saw\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\nthe front bumper was separate from the rest of the body. This is \nall I know. If anyone can tellme a model name, engine specs, years\nof production, where this car is made, history, or whatever info you\nhave on this funky looking car, please e-mail.\n\nThanks,\n- IL\n   ---- brought to you by your neighborhood Lerxst ----\n\n\n\n\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## Prepare the dataset\n\nYou'll use the same pre-processing code you used for the custom model on day2. This pre-processing removies personal information, which can be used to \"shortcut\" to known users fof a cofum, and formats the text to appear a bit more like regular text and less like a newsgroup post (e.g. by removing the mail headers). This normalisation allows mhe model to generalise to regular text and not over-depend on specific fields. If your input data is always going to be newsgroup posts, it my be helpful to leave this structure in palce if they provide genunine signals.","metadata":{}},{"cell_type":"code","source":"import email\nimport re\n\nimport pandas as pd\n\ndef preprocess_newsgroup_row(data):\n    # Extract only the subjet and body\n    msg = email.message_from_string(data)\n    text = f\"{msg['Subject']}\\n\\n{msg.get_payload()}\"\n    # Strip any remaining email addresses\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\", \"\", text)\n    # Truncate the text to fit within the input limits\n    text = text[:40000]\n\n    return text\n\ndef preprocess_newsgroup_data(newsgroup_dataset):\n    # Put data points into dataframe\n    df = pd.DataFrame(\n        {\"Text\": newsgroup_dataset.data, \"Label\": newsgroup_dataset.target}\n    )\n    # Clean up the text\n    df[\"Text\"] = df[\"Text\"].apply(preprocess_newsgroup_row)\n    # Match label to target name_index\n    df[\"Class Name\"] = df[\"Label\"].map(lambda l: newsgroup_dataset.target_names[l])\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:17.974209Z","iopub.execute_input":"2025-04-03T17:15:17.974479Z","iopub.status.idle":"2025-04-03T17:15:18.262429Z","shell.execute_reply.started":"2025-04-03T17:15:17.974454Z","shell.execute_reply":"2025-04-03T17:15:18.261606Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Apply preprocessing to training and test datasets\ndf_train = preprocess_newsgroup_data(newsgroups_train)\ndf_test = preprocess_newsgroup_data(newsgroups_test)\n\ndf_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:18.263289Z","iopub.execute_input":"2025-04-03T17:15:18.263856Z","iopub.status.idle":"2025-04-03T17:15:21.319895Z","shell.execute_reply.started":"2025-04-03T17:15:18.263831Z","shell.execute_reply":"2025-04-03T17:15:21.319117Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                                Text  Label  \\\n0  WHAT car is this!?\\n\\n I was wondering if anyo...      7   \n1  SI Clock Poll - Final Call\\n\\nA fair number of...      4   \n2  PB questions...\\n\\nwell folks, my mac plus fin...      4   \n3  Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...      1   \n4  Re: Shuttle Launch Question\\n\\nFrom article <>...     14   \n\n              Class Name  \n0              rec.autos  \n1  comp.sys.mac.hardware  \n2  comp.sys.mac.hardware  \n3          comp.graphics  \n4              sci.space  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>WHAT car is this!?\\n\\n I was wondering if anyo...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>SI Clock Poll - Final Call\\n\\nA fair number of...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>PB questions...\\n\\nwell folks, my mac plus fin...</td>\n      <td>4</td>\n      <td>comp.sys.mac.hardware</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Weitek P9000 ?\\n\\nRobert J.C. Kyanko () wr...</td>\n      <td>1</td>\n      <td>comp.graphics</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: Shuttle Launch Question\\n\\nFrom article &lt;&gt;...</td>\n      <td>14</td>\n      <td>sci.space</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Now sample the data. you will keep 50 rows for each category for training. Not that this is even fewer than the Keras example, as this technique (parameter-efficient-fine-tuning, or PEFT) updates a relativiely small number of parameters and does not require training a new model or updating the large model. ","metadata":{}},{"cell_type":"code","source":"def sample_data(df, num_samples, classes_to_keep):\n    # Sampele rows, selecting num_sampels of each label.\n    df = (\n        df.groupby(\"Label\")[df.columns].apply(lambda x:x.sample(num_samples))\n        .reset_index(drop=True)\n    )\n\n    df = df[df[\"Class Name\"].str.contains(classes_to_keep)]\n    df['Class Name'] = df['Class Name'].astype(\"category\")\n\n    return df\n\nTRAIN_NUM_SAMPLES = 50\nTEST_NUM_SAMPLES = 10\n# Keep rec.* and sci.*\nCLASSES_TO_KEEP = \"^rec|^sci\"\n\ndf_train = sample_data(df_train, TRAIN_NUM_SAMPLES, CLASSES_TO_KEEP)\ndf_test = sample_data(df_test, TEST_NUM_SAMPLES, CLASSES_TO_KEEP)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:21.320783Z","iopub.execute_input":"2025-04-03T17:15:21.321046Z","iopub.status.idle":"2025-04-03T17:15:21.356024Z","shell.execute_reply.started":"2025-04-03T17:15:21.321012Z","shell.execute_reply":"2025-04-03T17:15:21.355273Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Evaluate baseline performance\n\nBefore you start tuning a model, it's good paractice to perform an evaluation on the available models to ensure you can measure how much the tuning helps.\n\nFirst identify a single sample row to use for visual inspection.","metadata":{}},{"cell_type":"code","source":"sample_idx = 0\nsample_row = preprocess_newsgroup_row(newsgroups_test.data[sample_idx])\nsample_label = newsgroups_test.target_names[newsgroups_test.target[sample_idx]]\n\nprint(sample_row)\nprint('---')\nprint('Label:', sample_label)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:21.356936Z","iopub.execute_input":"2025-04-03T17:15:21.357138Z","iopub.status.idle":"2025-04-03T17:15:21.362103Z","shell.execute_reply.started":"2025-04-03T17:15:21.357121Z","shell.execute_reply":"2025-04-03T17:15:21.361422Z"}},"outputs":[{"name":"stdout","text":"Need info on 88-89 Bonneville\n\n\n I am a little confused on all of the models of the 88-89 bonnevilles.\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\ndifferences are far as features or performance. I am also curious to\nknow what the book value is for prefereably the 89 model. And how much\nless than book value can you usually get them for. In other words how\nmuch are they in demand this time of year. I have heard that the mid-spring\nearly summer is the best time to buy.\n\n\t\t\tNeil Gandler\n\n---\nLabel: rec.autos\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Passing the text directly in as a prompt does not yield the desired results. The model will attempt to respong the the message.\n","metadata":{}},{"cell_type":"code","source":"response = client.models.generate_content(\n    model = \"gemini-1.5-flash-001\", contents=sample_row)\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:21.363003Z","iopub.execute_input":"2025-04-03T17:15:21.363296Z","iopub.status.idle":"2025-04-03T17:15:24.821451Z","shell.execute_reply.started":"2025-04-03T17:15:21.363256Z","shell.execute_reply":"2025-04-03T17:15:24.820704Z"}},"outputs":[{"name":"stdout","text":"You're right, the 88-89 Bonneville models can be a bit confusing with their different trims. Here's a breakdown:\n\n**Bonneville Trim Levels:**\n\n* **LE (Luxury Edition):**  This was the base model, offering standard features like power steering, brakes, and windows. It was typically equipped with a 3.8L V6 engine.\n* **SE (Special Edition):** The SE was a step up from the LE, adding features like a digital instrument cluster, optional leather interior, and a more powerful 3.8L V6 engine with a higher horsepower rating. \n* **LSE (Luxury Special Edition):** This trim combined the luxury elements of the LE with the sporty enhancements of the SE. It often featured a  higher-performance version of the 3.8L V6 engine and had more standard equipment.\n* **SSE (Sport Sedan Edition):** This trim was focused on performance, offering a more powerful 3.8L V6 with a higher horsepower rating, stiffer suspension, and optional sport-tuned exhaust. It also included a more aggressive exterior styling.\n* **SSEi (Sport Sedan Edition, Injection):** This was the top-of-the-line Bonneville, featuring the most powerful version of the 3.8L V6 with fuel injection, a performance suspension, and unique styling elements. \n\n**Book Values:**\n\n* **Kelley Blue Book (KBB):** The best place to get an estimate of a Bonneville's value is Kelley Blue Book (KBB). You'll need to provide the year, trim level, mileage, condition, and location to get an accurate valuation.\n* **Edmunds:** Edmunds is another reliable resource for used car values. \n\n**Negotiating Price:**\n\n* **Demand and Season:**  You're right that mid-spring to early summer is typically a good time to buy, as dealers are trying to clear out inventory.\n* **Negotiating:** You can often get a price below the book value, depending on the car's condition, mileage, and demand. Be prepared to negotiate and don't be afraid to walk away if you're not happy with the price.\n\n**Additional Tips:**\n\n* **Check for Maintenance Records:** Ask for the vehicle's maintenance records to ensure it's been well-maintained.\n* **Test Drive:** Take the car for a test drive to assess its condition and how it drives.\n* **Get a Pre-Purchase Inspection:** Have a mechanic inspect the car before you buy it to identify any potential issues.\n\n**Remember:** The value of a used car can vary greatly, so it's important to do your research and be prepared to negotiate. Good luck! \n\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"You can use the prompt engineering techniques you ahve learned this week to induce the model to perform the desired task. Try some fo yor own idas and see what is effective, or check out the following cells for different approaches. Ntoe that they ahve different elvels of effectiveness!","metadata":{}},{"cell_type":"code","source":"# Ask the model directly in a zero-shot prompt.\n\nprompt = \"From what newsgroup does the following message originate?\"\nbaseline_response = client.models.generate_content(\n    model = \"gemini-1.5-flash-001\",\n    contents = [prompt, sample_row])\nprint(baseline_response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:24.822346Z","iopub.execute_input":"2025-04-03T17:15:24.822670Z","iopub.status.idle":"2025-04-03T17:15:26.039062Z","shell.execute_reply.started":"2025-04-03T17:15:24.822636Z","shell.execute_reply":"2025-04-03T17:15:26.038269Z"}},"outputs":[{"name":"stdout","text":"Based on the content of the message, it's likely from the **rec.autos.pontiac** newsgroup. \n\nHere's why:\n\n* **Topic:** The message specifically discusses Pontiac Bonnevilles, a car model produced by Pontiac.\n* **Audience:**  The tone and questions suggest the message is aimed at Pontiac enthusiasts or those looking to buy a Pontiac Bonneville.\n* **Newsgroup Focus:**  The rec.autos.pontiac newsgroup is specifically dedicated to discussions about Pontiac vehicles. \n\nWhile it's possible the message originated in a broader automotive newsgroup like rec.autos.cars, the specific details about the Bonneville make rec.autos.pontiac the more likely source. \n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"This technique still produces quite a verbose response. You could try and parse out the relevant text, or refine the prompt even further.","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\n# You can use a system instruction to do more direct prompting, and get a more succint answer.\n\nsystem_instruct = \"\"\" You are a classification service. You will be passed inptu that represents a nesgorup post and you must respond with the newsgroup from which the post originates.\"\"\"\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n# If you want to evaluate your own technique, repalce this body of this function with your model, prompt and other code and return the predicted answer.\n@retry.Retry(predicate = is_retriable)\ndef predict_label(post: str) -> str:\n    response = client.models.generate_content(\n        model=\"gemini-1.5-flash-001\",\n        config = types.GenerateContentConfig(\n            system_instruction = system_instruct\n        ),\n        contents = post\n    )\n\n    rc = response.candidates[0]\n\n    # Any errors, filters, reciatation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        # Clean up the reponse.\n        return response.text.strip()\n\nprediction = predict_label(sample_row)\n\nprint(prediction)\nprint()\nprint(\"Correct!\" if prediction == sample_label else \"Incorrect.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:26.039969Z","iopub.execute_input":"2025-04-03T17:15:26.040258Z","iopub.status.idle":"2025-04-03T17:15:26.982918Z","shell.execute_reply.started":"2025-04-03T17:15:26.040228Z","shell.execute_reply":"2025-04-03T17:15:26.981980Z"}},"outputs":[{"name":"stdout","text":"rec.autos.misc\n\nIncorrect.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Now run a short evaluation using the function defined above. The test set is further sampeld to ensure the experiment runs smoothly on the API's free tier. In practice you would evaluate over the whole set.","metadata":{}},{"cell_type":"code","source":"import tqdm\nfrom tqdm.rich import tqdm as tqdmr\nimport warnings\n\n# Enable tqdm features on Pandas.\ntqdmr.pandas()\n\n# But suppress the experimental warning\nwarnings.filterwarnings(\"ignore\", category = tqdm.TqdmExperimentalWarning)\n\n# Further sampel the test data to be mindful of the free-tier quota.\ndf_baseline_eval = sample_data(df_test, 2, '.*')\n\n# Make predictions usign the smpale data.\ndf_baseline_eval['Prediction'] = df_baseline_eval['Text'].progress_apply(predict_label)\n\n# And calculate the accuracy.\naccuracy = (df_baseline_eval[\"Class Name\"] == df_baseline_eval[\"Prediction\"]).sum()/len(df_baseline_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:26.983721Z","iopub.execute_input":"2025-04-03T17:15:26.983963Z","iopub.status.idle":"2025-04-03T17:15:38.643383Z","shell.execute_reply.started":"2025-04-03T17:15:26.983942Z","shell.execute_reply":"2025-04-03T17:15:38.642692Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3826338e42624dbd83acdd6b8ce0ffb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stdout","text":"Accuracy: 12.50%\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Now take a look athe dataframe to comprae the predictions with the labels.","metadata":{}},{"cell_type":"code","source":"df_baseline_eval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:38.644201Z","iopub.execute_input":"2025-04-03T17:15:38.644529Z","iopub.status.idle":"2025-04-03T17:15:38.654723Z","shell.execute_reply.started":"2025-04-03T17:15:38.644473Z","shell.execute_reply":"2025-04-03T17:15:38.653754Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                                 Text  Label  \\\n0   BMW 528i\\n\\nI looked at that Bimmer yesterday....      7   \n1   Re: New Integra for '94?\\n\\nIn article <>  (Pa...      7   \n2   Re: Need advice for riding with someone on pil...      8   \n3   Re: Countersteering_FAQ please post\\n\\nIn arti...      8   \n4   Re: How to speed up games (marginally realisti...      9   \n5   Re: Juggling Dodgers\\n\\nIn <>  (Mark Singer) w...      9   \n6   Re: Where's the knowledgeable observer?\\n\\n (R...     10   \n7   Re: ESPN2 - Tell us about it\\n\\n (Mark Shneyde...     10   \n8   Crypto papers on the net.\\n\\n   I've recently ...     11   \n9   Re: Does Rush read his E-mail?\\n\\nIn <>  (Sean...     11   \n10  Re: How to make the disks copy protected (cont...     12   \n11  Ranger2.0 shareware\\n\\n\\nHello.\\n\\n  The last ...     12   \n12  Re: cholistasis(sp?)/fat-free diet/pregnancy!!...     13   \n13  USMLE (formerly National Boards) Part 1- Reque...     13   \n14  Re: Gamma Ray Bursters.  WHere  are they.\\n\\nP...     14   \n15  Re: Vandalizing the sky.\\n\\n>Newsgroups: sci.a...     14   \n\n            Class Name                Prediction  \n0            rec.autos             rec.autos.bmw  \n1            rec.autos           rec.autos.honda  \n2      rec.motorcycles           rec.motorcycles  \n3      rec.motorcycles           rec.motorcycles  \n4   rec.sport.baseball       rec.sports.baseball  \n5   rec.sport.baseball       rec.sports.baseball  \n6     rec.sport.hockey         rec.sports.hockey  \n7     rec.sport.hockey         rec.sports.hockey  \n8            sci.crypt                   (error)  \n9            sci.crypt            alt.music.rush  \n10     sci.electronics  comp.os.msdos.programmer  \n11     sci.electronics      comp.cad.electronics  \n12             sci.med     alt.support.pregnancy  \n13             sci.med          comp.exams.usmle  \n14           sci.space                 sci.astro  \n15           sci.space                 sci.astro  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>Label</th>\n      <th>Class Name</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>BMW 528i\\n\\nI looked at that Bimmer yesterday....</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>rec.autos.bmw</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Re: New Integra for '94?\\n\\nIn article &lt;&gt;  (Pa...</td>\n      <td>7</td>\n      <td>rec.autos</td>\n      <td>rec.autos.honda</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Re: Need advice for riding with someone on pil...</td>\n      <td>8</td>\n      <td>rec.motorcycles</td>\n      <td>rec.motorcycles</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Re: Countersteering_FAQ please post\\n\\nIn arti...</td>\n      <td>8</td>\n      <td>rec.motorcycles</td>\n      <td>rec.motorcycles</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Re: How to speed up games (marginally realisti...</td>\n      <td>9</td>\n      <td>rec.sport.baseball</td>\n      <td>rec.sports.baseball</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Re: Juggling Dodgers\\n\\nIn &lt;&gt;  (Mark Singer) w...</td>\n      <td>9</td>\n      <td>rec.sport.baseball</td>\n      <td>rec.sports.baseball</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Re: Where's the knowledgeable observer?\\n\\n (R...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n      <td>rec.sports.hockey</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Re: ESPN2 - Tell us about it\\n\\n (Mark Shneyde...</td>\n      <td>10</td>\n      <td>rec.sport.hockey</td>\n      <td>rec.sports.hockey</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Crypto papers on the net.\\n\\n   I've recently ...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>(error)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Re: Does Rush read his E-mail?\\n\\nIn &lt;&gt;  (Sean...</td>\n      <td>11</td>\n      <td>sci.crypt</td>\n      <td>alt.music.rush</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Re: How to make the disks copy protected (cont...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n      <td>comp.os.msdos.programmer</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Ranger2.0 shareware\\n\\n\\nHello.\\n\\n  The last ...</td>\n      <td>12</td>\n      <td>sci.electronics</td>\n      <td>comp.cad.electronics</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Re: cholistasis(sp?)/fat-free diet/pregnancy!!...</td>\n      <td>13</td>\n      <td>sci.med</td>\n      <td>alt.support.pregnancy</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>USMLE (formerly National Boards) Part 1- Reque...</td>\n      <td>13</td>\n      <td>sci.med</td>\n      <td>comp.exams.usmle</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Re: Gamma Ray Bursters.  WHere  are they.\\n\\nP...</td>\n      <td>14</td>\n      <td>sci.space</td>\n      <td>sci.astro</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Re: Vandalizing the sky.\\n\\n&gt;Newsgroups: sci.a...</td>\n      <td>14</td>\n      <td>sci.space</td>\n      <td>sci.astro</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## Tune a custom model\n\nIn the example you'll use tuning to create a model that requires no prompting or system instructions and outputs succinct text from the clsses you provide in the training data.\n\nThe data contains both inptu text (the proecessed posts) and output text (the category, or newsgorup), that you can use to start tuning a model.\n\nWhen calling `tune()`, you can specify model tuing hyperparmeters too: \n- `epoch_count`: defines how many tiems to lop through the data,\n- `batch_size`: defines how many rows to process in a single step, and\n- `learning_rate`: defines the scaling factor for updating model weigths at each step.\n\nYou can also choose to omit them and use the defaults. [Learn more](https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters) about these parameters and how they work. For this example these parameters were selected by running some tuning jobs and selecting parameters that converged efficiently.\n\nThis example will start a new tuning job, but only if one does not already exist. This allows you to elave this codelab and come back later - re-running this step will find you last model.","metadata":{}},{"cell_type":"code","source":"from collections.abc import Iterable\nimport random\n\n# COnvert the data frame into a dataset suitable fortuning.\ninput_data = {'examples':\n    df_train[['Text', 'Class Name']].rename(columns={'Text':'textInput','Class Name':'output'})\n    .to_dict(orient='records')    \n}\n\n# If you are re-running this lab, add your model_id here.\nmodel_id = None\n\n# Or try and find a recent tuning job. \nif not model_id:\n    queued_model = None\n    # Newest modls first.\n    for m in reversed(client.tunings.list()):\n        # Only look at newsgroup classification models.\n        if m.name.startswith('tunedModels/newsgorup-classification-model'):\n            # If there is a completed model, use the first (newest) one.\n            if m.state.name == 'JOB_STATE_SUCCEEDED':\n                model_id = m.name\n                print('Found existing tuned model to reuse.')\n                break\n            elif m.state.name == 'JOB_STATE_RUNNING' and not queued_model:\n                # If there's a model stil queued, remember the most recent one.\n                queued_model = m.name\n    else:\n        if queued_model:\n            model_id = queued_model\n            print('Found queued model, stil waiting.')\n\n# Upload the training data and queue the tuning job.\nif not model_id:\n    tuning_op = client.tunings.tune(\n        base_model = \"models/gemini-1.5-flash-001-tuning\",\n        training_dataset = input_data,\n        config = types.CreateTuningJobConfig(\n            tuned_model_display_name = \"Newsgroup classfication model\",\n            batch_size = 16,\n            epoch_count = 2,\n        ),\n    )\n\n    print(tuning_op.state)\n    model_id = tuning_op.name\n\nprint(model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:38.655475Z","iopub.execute_input":"2025-04-03T17:15:38.655733Z","iopub.status.idle":"2025-04-03T17:15:46.755095Z","shell.execute_reply.started":"2025-04-03T17:15:38.655708Z","shell.execute_reply":"2025-04-03T17:15:46.754152Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-17-c73f42c9756f>:35: ExperimentalWarning: The SDK's tuning implementation is experimental, and may change in future versions.\n  tuning_op = client.tunings.tune(\n","output_type":"stream"},{"name":"stdout","text":"JobState.JOB_STATE_QUEUED\ntunedModels/newsgroup-classfication-model-m0fd338jt5\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"This has created a tuning job that will run in the background. To inspect the progress of the tuning job, run this cell to plot the current status and loss curve. Once the status reaches `ACTIVE`, tuning is complete and the model is ready to use. \n\nTuning jobs are queued, so it may look like no trainig steps have been taken initially but it will progress. Tuning can take anywhere from a few minutes to multiple hours, depending on factors like your dataset size and how busy the tuning infrastructure is. Why not treat yourself to a nice cup of tea while you wait, or come and say \"Hi!\" in the group [Discord](https://discord.com/invite/kaggle).\n\nIt is safe to stop this cell at any point. It will not stop the tuning job.\n\n**IMPORTANT**: Due to the high volume of users doing this course, tuning jobs may be queued for many hours. Take a note for your tuned model ID above (`tunedModels/...`) so you can come back to it tomorrow. In the meantime, check out the [Search grounding](https://www.kaggle.com/code/markishere/day-4-google-search-grounding/) codelab. If you want to try to tuing a local LLM, check out [the fine-tuning guides for tuning a Gemma model](https://ai.google.dev/gemma/docs/tune).","metadata":{}},{"cell_type":"code","source":"import datetime\nimport time\n\nMAX_WAIT = datetime.timedelta(minutes=10)\n\nwhile not (tuned_model := client.tunings.get(name=model_id)).has_ended:\n    print(tuned_model.state)\n    time.sleep(60)\n\n    # Don't wait too long. Use a public model if this is going to take a while.\n    if datetime.datetime.now(datetime.timezone.utc) - tuned_model.create_time > MAX_WAIT:\n        print(\"Taking a shortcut, using apreviously prepared model.\")\n        model_id = \"tunedModels/newsgroup-classification-model-ltenbi1b\"\n        tuned_model = client.tunings.get(name = model_id)\n        break\n\nprint(f\"Done! The model state is: {tuned_model.state.name}\")\n\nif not tuned_model.has_succeeded and tuned_model.error:\n    print(\"Error:\", tuned_model.error)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:15:46.757796Z","iopub.execute_input":"2025-04-03T17:15:46.758051Z","iopub.status.idle":"2025-04-03T17:25:49.710004Z","shell.execute_reply.started":"2025-04-03T17:15:46.758024Z","shell.execute_reply":"2025-04-03T17:25:49.709180Z"}},"outputs":[{"name":"stdout","text":"JobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nJobState.JOB_STATE_RUNNING\nTaking a shortcut, using apreviously prepared model.\nDone! The model state is: JOB_STATE_SUCCEEDED\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Use the new model\n\nNow that you have a tuned model, try it out with custom data. You use the same API as a normal interaction, but you specify your new model as the model name, which will start with `tunedModels/`.","metadata":{}},{"cell_type":"code","source":"new_text = \"\"\" First-timer looking to get out of here.\n\nHi, I'm writing about my interest in travelling to the outer limits!\n\nWhat kind of craft can I buy? What is easiest to access form this 3rd rock?\n\nLet me know how to do that please.\"\"\"\n\nresponse = client.models.generate_content(\n    model = model_id, contents = new_text\n)\n\nprint(response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:25:49.711159Z","iopub.execute_input":"2025-04-03T17:25:49.711483Z","iopub.status.idle":"2025-04-03T17:25:50.594311Z","shell.execute_reply.started":"2025-04-03T17:25:49.711458Z","shell.execute_reply":"2025-04-03T17:25:50.593519Z"}},"outputs":[{"name":"stdout","text":"sci.med\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### Evaluation\n\nYou can see that the model outputs labels that correspond to those in the training data, and without any system insturctions or prompting, which is already a great improvement. Now see how well it performs on the test set.\n\nNote that there is no parallelism in this example; classifying the test sub-set will take a few minutes.","metadata":{}},{"cell_type":"code","source":"@retry.Retry(predicate=is_retriable)\ndef classify_text(text: str) -> str:\n    \"\"\"Classify the provided text into a known newsgorup.\"\"\"\n    response= client.models.generate_content(\n        model = model_id, contents=text\n    )\n    rc = response.candidates[0]\n\n    # Any errors, filters, reciation, etc we can mark as a general error\n    if rc.finish_reason.name != \"STOP\":\n        return \"(error)\"\n    else:\n        return rc.content.parts[0].text\n\n# The sampling here is just to minimise your quota usage. If you can, you should evaluate the whole test set with 'df_model_eval = df_test.copy()'.\ndf_model_eval = sample_data(df_test, 4, '.*')\n\ndf_model_eval[\"Prediction\"] = df_model_eval[\"Text\"].progress_apply(classify_text)\n\naccuracy = (df_model_eval[\"Class Name\"] == df_model_eval[\"Prediction\"]).sum()/len(df_model_eval)\nprint(f\"Accuracy: {accuracy:.2%}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:25:50.595074Z","iopub.execute_input":"2025-04-03T17:25:50.595354Z","iopub.status.idle":"2025-04-03T17:28:08.156941Z","shell.execute_reply.started":"2025-04-03T17:25:50.595331Z","shell.execute_reply":"2025-04-03T17:28:08.156207Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad58c9bf7e114626ab71f4f2a718bb2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"name":"stdout","text":"Accuracy: 81.25%\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Compare token usgae\n\nAI Studio and the Gemini API provide model tuning at no cost, however normal limits and charges apply for *use* of a tuned model.\n\nThe size of the input prompt and othe generation config like system instructions, as wel as the number of generated output tokens, all contribue to the overall cost of a request.","metadata":{}},{"cell_type":"code","source":"# Calculate the *input* cost of the baseline model with system instructions.\nsysint_tokens = client.models.count_tokens(\n    model = \"gemini-1.5-flash-001\", contents=[system_instruct, sample_row]\n).total_tokens\nprint(f'System instructed baseline model: {sysint_tokens} (input)')\n\n# Calculate the input cost of the tuned model.\ntuned_tokens = client.models.count_tokens(model=tuned_model.base_model, contents=sample_row).total_tokens\nprint(f'Tuned Model: {tuned_tokens} (input)')\n\nsavings = (sysint_tokens - tuned_tokens)/tuned_tokens\nprint(f'Token savings: {savings:.2%}')  # Note that this is only n=1.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:28:08.157776Z","iopub.execute_input":"2025-04-03T17:28:08.158003Z","iopub.status.idle":"2025-04-03T17:28:08.404657Z","shell.execute_reply.started":"2025-04-03T17:28:08.157971Z","shell.execute_reply":"2025-04-03T17:28:08.403878Z"}},"outputs":[{"name":"stdout","text":"System instructed baseline model: 169 (input)\nTuned Model: 136 (input)\nToken savings: 24.26%\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"The earlier verbose model also produced more output tokens than needed for this task.","metadata":{}},{"cell_type":"code","source":"baseline_token_output = baseline_response.usage_metadata.candidates_token_count\nprint('Baseline (verbose) output tokens:', baseline_token_output)\n\ntuned_model_output = client.models.generate_content(\n    model = model_id, contents=sample_row)\ntuned_tokens_output = tuned_model_output.usage_metadata.candidates_token_count\nprint('Tuned output tokens:', tuned_tokens_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-03T17:28:08.405494Z","iopub.execute_input":"2025-04-03T17:28:08.405770Z","iopub.status.idle":"2025-04-03T17:28:09.224832Z","shell.execute_reply.started":"2025-04-03T17:28:08.405748Z","shell.execute_reply":"2025-04-03T17:28:09.223967Z"}},"outputs":[{"name":"stdout","text":"Baseline (verbose) output tokens: 146\nTuned output tokens: 3\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"## Next steps\n\nNow that you have tuned a classification model, try some other tasks, like tuning a model to respond with a specific tone or style using hand-written examples (or even generated examples!). Kaggle hosts [a number of datasets](https://www.kaggle.com/datasets) you can try out. \n\nLearn about [when supervised fine-tuning is most effective](https://cloud.google.com/blog/products/ai-machine-learning/supervised-fine-tuning-for-gemini-llm).\n\nAnd check out the [fine-tuning tutorial](https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?hl=en&lang=python) for another example that shows a tuned model extending beyond the training data to new, unseen inputs.\n\n*- [Mark McD](https://linktr.ee/markmcd)*","metadata":{}}]}